{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Gradient\n",
    "\n",
    "Here we will consider linear regression for data set with single variable.\n",
    "We will look into\n",
    "- Model function\n",
    "- Cost function\n",
    "- Gradient descent algorithm for calculating Parameters\n",
    "for Linear gradient with single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function\n",
    "\n",
    "$$ f_{w,B}(x_{i})=w x_{i}\\:+\\:B $$\n",
    "\n",
    "- $ f_{w,B}(x_{i}) $ is the model output ( $ \\hat{y} $ ).\n",
    "- $x_{i}$ is the input data\n",
    "- $w$ is the weight.\n",
    "- $B$ is the bias.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(X,w,B):\n",
    "    '''\n",
    "    X is a numpy array of shape (n,1)\n",
    "    w,B are scalars\n",
    "    ruturs Y_hat, numpy array of shape (n,1)\n",
    "    \n",
    "    '''\n",
    "    Y_hat= X*w + B  #calculate the predicted values\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "$$ J(w,B)= \\frac{1}{2m}\\sum \\limits _{i=1} ^{m} [f_{w,B}(x^{(i)})-y^{(i)}]^{2} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(X,Y,w,B):\n",
    "    '''\n",
    "    This function calculates the cost function for the given data and model parameters\n",
    "    '''\n",
    "    m=X.shape[0]\n",
    "    cost = np.sum((Y-X*w-B)**2)/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "We need to find the parameters w,B to fit our estimation with the actual values.  \n",
    "We can find the best approximation for w,B  by minimizing the cost function.  \n",
    "To reach minimum of Cost function, iterative technique called *Gradient Descent Algorithm* is used.  \n",
    "  \n",
    "Let's have a look at gradient descent algorithm.  \n",
    "$$ w_{n}= w_{n-1} - \\alpha \\frac{\\partial{J_{w}}}{\\partial{w}} $$  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
